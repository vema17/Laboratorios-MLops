{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afcad3e3-516c-444d-9c34-c4d21d2fe488",
   "metadata": {},
   "source": [
    "# Laboratorio 30/05: Tarea de Metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cba9a5-11f7-4313-a28c-a9fc3010712c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset escogido: Vinos. Es extraido de scikit-learn.org usando su libreria de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f7ee8e1-4e69-4b62-8877-e0331c646d33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importación de librerías necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5714ad4e-1fb8-4dc4-a44a-e67188f7f891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cargar un conjunto de datos de ejemplo\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1417f26f-e757-458d-bb6d-3e5af5b12bb2",
   "metadata": {},
   "source": [
    "## Métricas de Evaluación de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61538fcd-1d17-470c-b76f-8dde41701816",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall (micro): 1.00\n",
      "Precision (micro): 1.00\n",
      "F1 Score (micro): 1.00\n",
      "Recall (macro): 1.00\n",
      "Precision (macro): 1.00\n",
      "F1 Score (macro): 1.00\n",
      "Recall (weighted): 1.00\n",
      "Precision (weighted): 1.00\n",
      "F1 Score (weighted): 1.00\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular métricas\n",
    "\n",
    "recall = recall_score(y_test, y_pred, average='micro')\n",
    "precision = precision_score(y_test, y_pred, average='micro')\n",
    "f1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "print(f\"Recall (micro): {recall:.2f}\")\n",
    "print(f\"Precision (micro): {precision:.2f}\")\n",
    "print(f\"F1 Score (micro): {f1:.2f}\")\n",
    "\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Recall (macro): {recall:.2f}\")\n",
    "print(f\"Precision (macro): {precision:.2f}\")\n",
    "print(f\"F1 Score (macro): {f1:.2f}\")\n",
    "\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Recall (weighted): {recall:.2f}\")\n",
    "print(f\"Precision (weighted): {precision:.2f}\")\n",
    "print(f\"F1 Score (weighted): {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0543e23-9721-4128-be8c-b1c73593e461",
   "metadata": {},
   "source": [
    "### Esto puede deberse a varias razones, como la naturaleza del dataset wine, que podría ser fácilmente separable con un clasificador como Random Forest, o que el conjunto de prueba sea relativamente pequeño y sencillo de clasificar. Probaremos otro clasificador: LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ce936ea-b210-44e3-a79b-4e2f4ee784b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall (micro): 1.00\n",
      "Precision (micro): 1.00\n",
      "F1 Score (micro): 1.00\n",
      "Recall (macro): 1.00\n",
      "Precision (macro): 1.00\n",
      "F1 Score (macro): 1.00\n",
      "Recall (weighted): 1.00\n",
      "Precision (weighted): 1.00\n",
      "F1 Score (weighted): 1.00\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular métricas\n",
    "\n",
    "recall = recall_score(y_test, y_pred, average='micro')\n",
    "precision = precision_score(y_test, y_pred, average='micro')\n",
    "f1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "print(f\"Recall (micro): {recall:.2f}\")\n",
    "print(f\"Precision (micro): {precision:.2f}\")\n",
    "print(f\"F1 Score (micro): {f1:.2f}\")\n",
    "\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Recall (macro): {recall:.2f}\")\n",
    "print(f\"Precision (macro): {precision:.2f}\")\n",
    "print(f\"F1 Score (macro): {f1:.2f}\")\n",
    "\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Recall (weighted): {recall:.2f}\")\n",
    "print(f\"Precision (weighted): {precision:.2f}\")\n",
    "print(f\"F1 Score (weighted): {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b506a888-e834-4b14-8d46-643e8e1e8c7a",
   "metadata": {},
   "source": [
    "### Parece que los resultados con LogisticRegression también son perfectos. Esto podría deberse a la naturaleza del dataset y la separación clara entre las clases en el conjunto de datos wine. Vamos a probar con SVC (Support Vector Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5300f160-8d06-4e3f-9684-123aec110d84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall (micro): 0.81\n",
      "Precision (micro): 0.81\n",
      "F1 Score (micro): 0.81\n",
      "Recall (macro): 0.76\n",
      "Precision (macro): 0.77\n",
      "F1 Score (macro): 0.76\n",
      "Recall (weighted): 0.81\n",
      "Precision (weighted): 0.80\n",
      "F1 Score (weighted): 0.80\n"
     ]
    }
   ],
   "source": [
    "model = SVC(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular métricas\n",
    "\n",
    "recall = recall_score(y_test, y_pred, average='micro')\n",
    "precision = precision_score(y_test, y_pred, average='micro')\n",
    "f1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "print(f\"Recall (micro): {recall:.2f}\")\n",
    "print(f\"Precision (micro): {precision:.2f}\")\n",
    "print(f\"F1 Score (micro): {f1:.2f}\")\n",
    "\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Recall (macro): {recall:.2f}\")\n",
    "print(f\"Precision (macro): {precision:.2f}\")\n",
    "print(f\"F1 Score (macro): {f1:.2f}\")\n",
    "\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Recall (weighted): {recall:.2f}\")\n",
    "print(f\"Precision (weighted): {precision:.2f}\")\n",
    "print(f\"F1 Score (weighted): {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f75dd-bef9-475f-93e0-3ddaecb34a11",
   "metadata": {},
   "source": [
    "### Estos resultados son más realistas y muestran que el modelo no es perfecto en la clasificación. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219ebd09-8adc-4bcc-b4d1-18035bcc315d",
   "metadata": {},
   "source": [
    "### Explicación de las Métricas de Evaluación y sus Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1398ccfe-ab03-4046-8f54-1e1e176ba872",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Precisión = \\frac{TP}{TP + FP}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Recall = \\frac{TP}{TP + FN}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle F1 Score = 2 \\cdot \\frac{Precisión \\cdot Recall}{Precisión + Recall}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Micro, Macro y Ponderado"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Micro:** Se calculan los verdaderos positivos, falsos positivos y falsos negativos globalmente y se aplican las fórmulas como si fuera un solo problema de clasificación binaria."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{Precisión Micro} = \\frac{\\sum_i \\text{TP}_i}{\\sum_i (\\text{TP}_i + \\text{FP}_i)}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{Recall Micro} = \\frac{\\sum_i \\text{TP}_i}{\\sum_i (\\text{TP}_i + \\text{FN}_i)}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{F1 Score Micro} = 2 \\cdot \\frac{\\text{Precisión Micro} \\cdot \\text{Recall Micro}}{\\text{Precisión Micro} + \\text{Recall Micro}}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Macro:** Se calculan las métricas para cada clase individualmente y luego se toma el promedio (sin ponderar por el tamaño de la clase)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{Precisión Macro} = \\frac{1}{N} \\sum_i \\text{Precisión}_i$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{Recall Macro} = \\frac{1}{N} \\sum_i \\text{Recall}_i$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{F1 Score Macro} = \\frac{1}{N} \\sum_i \\text{F1-Score}_i$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Ponderado (Weighted):** Similar al macro, pero cada métrica se pondera por el número de instancias de cada clase."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{Precisión Ponderada} = \\sum_i \\frac{n_i}{N} \\cdot \\text{Precisión}_i$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{Recall Ponderado} = \\sum_i \\frac{n_i}{N} \\cdot \\text{Recall}_i$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{F1 Score Ponderado} = \\sum_i \\frac{n_i}{N} \\cdot \\text{F1-Score}_i$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definiciones básicas\n",
    "def definiciones_basicas():\n",
    "    from IPython.display import display, Math\n",
    "    display(Math(r'Precisión = \\frac{TP}{TP + FP}'))\n",
    "    display(Math(r'Recall = \\frac{TP}{TP + FN}'))\n",
    "    display(Math(r'F1 Score = 2 \\cdot \\frac{Precisión \\cdot Recall}{Precisión + Recall}'))\n",
    "\n",
    "definiciones_basicas()\n",
    "\n",
    "# Enfoques para clasificación multiclase\n",
    "def enfoques_multiclase():\n",
    "    from IPython.display import display,Math, Markdown\n",
    "    \n",
    "    display(Markdown(\"### Micro, Macro y Ponderado\"))\n",
    "    \n",
    "    display(Markdown(\"**Micro:** Se calculan los verdaderos positivos, falsos positivos y falsos negativos globalmente y se aplican las fórmulas como si fuera un solo problema de clasificación binaria.\"))\n",
    "    display(Math(r'\\text{Precisión Micro} = \\frac{\\sum_i \\text{TP}_i}{\\sum_i (\\text{TP}_i + \\text{FP}_i)}'))\n",
    "    display(Math(r'\\text{Recall Micro} = \\frac{\\sum_i \\text{TP}_i}{\\sum_i (\\text{TP}_i + \\text{FN}_i)}'))\n",
    "    display(Math(r'\\text{F1 Score Micro} = 2 \\cdot \\frac{\\text{Precisión Micro} \\cdot \\text{Recall Micro}}{\\text{Precisión Micro} + \\text{Recall Micro}}'))\n",
    "\n",
    "    display(Markdown(\"**Macro:** Se calculan las métricas para cada clase individualmente y luego se toma el promedio (sin ponderar por el tamaño de la clase).\"))\n",
    "    display(Math(r'\\text{Precisión Macro} = \\frac{1}{N} \\sum_i \\text{Precisión}_i'))\n",
    "    display(Math(r'\\text{Recall Macro} = \\frac{1}{N} \\sum_i \\text{Recall}_i'))\n",
    "    display(Math(r'\\text{F1 Score Macro} = \\frac{1}{N} \\sum_i \\text{F1-Score}_i'))\n",
    "\n",
    "    display(Markdown(\"**Ponderado (Weighted):** Similar al macro, pero cada métrica se pondera por el número de instancias de cada clase.\"))\n",
    "    display(Math(r'\\text{Precisión Ponderada} = \\sum_i \\frac{n_i}{N} \\cdot \\text{Precisión}_i'))\n",
    "    display(Math(r'\\text{Recall Ponderado} = \\sum_i \\frac{n_i}{N} \\cdot \\text{Recall}_i'))\n",
    "    display(Math(r'\\text{F1 Score Ponderado} = \\sum_i \\frac{n_i}{N} \\cdot \\text{F1-Score}_i'))\n",
    "\n",
    "enfoques_multiclase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf95f6-ed5f-43c1-8f93-f3d4a17ec621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
